{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/python-logging/\n",
    "logging.basicConfig(level = logging.DEBUG, filename='app.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Structure\n",
    "\n",
    "* Scrape sites\n",
    "    * Open page\n",
    "    * Look for a way to build a list\n",
    "    * Look for attributes within each list\n",
    "    * Combine into a dictionary\n",
    "    * Add dictionary to 'data' list\n",
    "* Build CSV\n",
    "\n",
    "Assume run every day from a server at 6AM - what implications does this have?\n",
    "* It means this will be run automatically without someone always watching it\n",
    "* No manual input\n",
    "* No error handling. The program has to be robust\n",
    "* Are there any implications on how recent CFPs should be?\n",
    "\n",
    "\n",
    "Assumptions\n",
    "* Journals won't have so many CFPs they spill over into more pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFP():\n",
    "    def __init__(self, journal, title, authors, due, link):\n",
    "        self.journal = journal\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.due = due\n",
    "        self.link = link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a list of dictionaries to build our dataframe.\n",
    "# This is more efficient than continuously 'appending' to our dataframe\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html\n",
    "data = []\n",
    "\n",
    "# helper function to scrape a site with a url\n",
    "# This is important because this script will be running automatically, so it needs to be robust\n",
    "# Error catching from https://stackoverflow.com/questions/16511337/correct-way-to-try-except-using-python-requests-module\n",
    "# Best practice for raising errors https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python\n",
    "\n",
    "def scraper(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        logging.warning(e)\n",
    "        for retry in range(10):\n",
    "            try:\n",
    "                time.sleep(60)\n",
    "                res = requests.get(url)\n",
    "            except requests.exceptions.Timeout:\n",
    "                logging.warning('Connection timed out')\n",
    "                continue\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(e)\n",
    "                raise\n",
    "            else:\n",
    "                break\n",
    "    except requests.exceptions.TooManyRedirects as e:\n",
    "        logging.error(e)\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logging.error(e)\n",
    "        raise\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "**TODO:**\n",
    "* Turn things into modularised functions\n",
    "* Add more testing to make sure sites have CFPs, etc.\n",
    "* Add error checking (what if they don't have a due date, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Systems Journal (ISJ)\n",
    "* https://onlinelibrary.wiley.com/journal/13652575\n",
    "* https://onlinelibrary.wiley.com/page/journal/13652575/homepage/special_issues.htm\n",
    "\n",
    "### Notes\n",
    "* Use Blair's downloaded HTML - Cloudflare blocks this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting HTML from moodle extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_isj():\n",
    "    l = []\n",
    "\n",
    "    # Snapshot from https://onlinelibrary.wiley.com/page/journal/13652575/homepage/special_issues.htm\n",
    "\n",
    "    html = open('Information Systems Journal.html', 'r')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Assume there can be 1 to arbitrary n rows in the table that holds our CFPs\n",
    "    # Assume the table will always have 2 columns, one for the paper, one for the due date\n",
    "\n",
    "    cfp_rows = soup.find('strong', string = 'Call for Papers').find_next('table').find_all('tr')\n",
    "\n",
    "    if cfp_rows:\n",
    "        for row in cfp_rows:\n",
    "            d = {}\n",
    "            d['Journal'] = 'Information Systems Journal'\n",
    "            d['URL'] = row.td.a['href']\n",
    "            d['Title'] = row.td.text\n",
    "            d['Due Date'] = dateparser.parse(row.find_all('td')[1].text)\n",
    "            l.append(d)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_isj())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Systems Research (ISR)\n",
    "* https://pubsonline.informs.org/journal/isre\n",
    "* https://pubsonline.informs.org/page/isre/calls-for-papers\n",
    "\n",
    "### Notes\n",
    "* There doesn't seem to be a convention for doing a \"due date\". The authors just do whatever they want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_isr():\n",
    "    l = []\n",
    "    base_url = 'https://pubsonline.informs.org'\n",
    "    cfp_announcement_url = base_url +'/page/isre/calls-for-papers'\n",
    "    \n",
    "    soup = scraper(cfp_announcement_url)\n",
    "    \n",
    "    # Only h1 is \"Calls for Papers\"\n",
    "    page_anchor = soup.find('h1')\n",
    "    \n",
    "    # Last 2 elements are unrelated - assume these elements are always going to be there\n",
    "    cfp_list = page_anchor.find_all_next('h2')[:-2]\n",
    "    \n",
    "    if cfp_list:\n",
    "        for cfp in cfp_list:\n",
    "            d = {}\n",
    "            d['Journal'] = 'Information Systems Research'\n",
    "            d['Title'] = cfp.text.strip()\n",
    "            # Assume researchers don't want verbosity\n",
    "            # d['Authors'] = '; '.join(cfp.find_next('h4', string = 'Special Issue Editors').next_sibling.next_sibling.text.split('\\n'))\n",
    "            d['Authors'] = '; '.join([researcher.split(' (')[0] for researcher in cfp.find_next('h4', string = 'Special Issue Editors').next_sibling.next_sibling.text.split('\\n')])\n",
    "            d['URL'] = base_url + cfp.find_next('a', href = re.compile('^/doi'))['href']\n",
    "            # Due/Submission date seems to be unstructured (no specific way of wording it)\n",
    "            # The assumptions is the words 'Submission' AND 'Due' or 'Deadline' will be there. Otherwise N/A\n",
    "            d['Due Date'] = dateparser.parse(scraper(d['URL']).find('td', string = re.compile('(Submission.*(Due|Deadline))|((Due|Deadline).*Submission)')).next_sibling.text)\n",
    "            l.append(d)\n",
    "    else:\n",
    "        logging.debug('ISR returned no CFPs')\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_isr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal of the Association for Information Systems (JAIS)\n",
    "* https://aisel.aisnet.org/jais/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_jais():\n",
    "    l = []\n",
    "    url = 'https://aisel.aisnet.org/jais/'\n",
    "    \n",
    "    soup = scraper(url)\n",
    "    \n",
    "    cfp_list = [cfp.next_sibling for cfp in soup.find_all(string = re.compile('Special Issue Call for Papers'))]\n",
    "    \n",
    "    if cfp_list:\n",
    "        for cfp in cfp_list:\n",
    "            d = {}\n",
    "            d['Journal'] = 'Journal of the Association for Information Systems'\n",
    "            d['URL'] = cfp['href']\n",
    "            d['Title'] = cfp.text.strip()\n",
    "            l.append(d)\n",
    "    else:\n",
    "        logging.debug('JAIS returned no CFPs')\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_jais())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal of Information Technology (JIT)\n",
    "* https://journals.sagepub.com/home/jina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_jit():\n",
    "    l = []\n",
    "    url = 'https://journals.sagepub.com/home/jina'\n",
    "    soup = scraper(url)\n",
    "\n",
    "    cfp_table = soup.find('h3', string = 'Call for Papers').find_next('table')\n",
    "    cfp_list = cfp_table.find_all('a')\n",
    "\n",
    "    if cfp_list:\n",
    "        for cfp in cfp_list:\n",
    "            d = {}\n",
    "            d['URL'] = cfp['href']\n",
    "            d['Journal'] = 'Journal of Information Technology'\n",
    "            d['Title'] = cfp.text.strip(\":“” \")\n",
    "            d['Due Date'] = dateparser.parse(re.match('^ First round submission deadline: (.+).$', cfp.next_sibling)[1])\n",
    "            l.append(d)\n",
    "    else:\n",
    "        logging.debug('JIT returned no CFPs')\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_jit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal of Strategic Information Systems (JSIS)\n",
    "### Note: It's actually Journal of Management Information Systems (JMIS)\n",
    "* https://jmis-web.org/issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_jmis():\n",
    "    l = []\n",
    "\n",
    "    url = 'https://jmis-web.org/issues'\n",
    "    soup = scraper(url)\n",
    "\n",
    "    cfp_list = [cfp for cfp in soup.find_all('a', class_ = 'alert-link') if cfp.previous_sibling == ' A new call for papers has been posted: ']\n",
    "\n",
    "    if cfp_list:\n",
    "        for cfp in cfp_list:\n",
    "            d = {}\n",
    "            d['Journal'] = 'Journal of Management Information Systems'\n",
    "            d['URL'] = 'https://jmis-web.org' + cfp['href']\n",
    "            d['Title'] = cfp.text\n",
    "            l.append(d)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_jmis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management Information Systems Quarterly (MISQ)\n",
    "* https://misq.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfp_misq():\n",
    "    l = []\n",
    "    \n",
    "    url = 'https://misq.org'\n",
    "    soup = scraper(url)\n",
    "\n",
    "    cfp_list = soup.find_all('a', string = re.compile('^Call for Papers:'))\n",
    "\n",
    "    cfp = cfp_list[0]\n",
    "\n",
    "    for cfp in cfp_list:\n",
    "        d = {}\n",
    "        d['Journal'] = 'Management Information Systems Quarterly'\n",
    "        d['URL'] = url + cfp['href']\n",
    "        d['Title'] = re.match('^Call for Papers:  (.*)$', cfp.text.strip())[1]\n",
    "        d['Due Date'] = dateparser.parse(re.match('^The submission deadline for this special issue is (.*)$', cfp.parent.next_sibling.next_sibling.text)[1])\n",
    "        l.append(d)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(get_cfp_misq())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/36107094/pandas-apply-to-all-values-except-missing\n",
    "# https://www.programiz.com/python-programming/datetime/strftime\n",
    "df['Due Date'] = df['Due Date'].apply(lambda x: str(x.strftime('%d/%m/%Y')) if pd.notnull(x) else 'Not Known')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {'Journal': 'Journal Name',\n",
    "              'URL': 'Link to CFP details page',\n",
    "              'Title': 'CFP title',\n",
    "              'Authors': 'CFP authors',\n",
    "              'Due Date': 'Due date',}\n",
    "df = df.rename(columns = rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_order = ['Journal Name', 'CFP title', 'CFP authors', 'Due date', 'Link to CFP details page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv', index = False, na_rep = 'N/A', columns = output_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
